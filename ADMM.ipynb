{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem definition and computing of derivatives\n",
    "This document is arranged like this :\n",
    "<ol>\n",
    "  <li>In the first part we compute the derivatives of logistic loss and regularized logistic loss </li>\n",
    "  <li>In the second part we solve the logistic regression problem for both cases presented in the first section using ADMM(an alternating direction method of multipliers) approach and we do the program related</li>\n",
    "  <li>In the third part we solve the logistic regression problem for both cases presented in the first section using BDMM(Basic differential method of multipliers) approach and we do the program related</li>\n",
    "  <li>We test both method using 2 datasets</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "Let be $S = \\{(x_i, y_i)_{i=1}^n \\}$ where $x_i \\in \\mathbb{R}^d, d$ is an integer and $y_i \\in \\{-1,1\\}$ $ \\forall i \\in \\{1,2,...n\\}$\n",
    "<br>\n",
    "<br>\n",
    "We define the following functions :\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}, x \\in \\mathbb{R}$$\n",
    "<br>\n",
    "$$h_w(x) = x^{T}\\bar{w}+w_0$$ with $ x, \\bar{w} \\in \\mathbb{R}^d, w_o \\in \\mathbb{R}$ Par la suite nous posons $w = (w_0, \\bar{w}) \\in \\mathbb{R}^{d+1}$\n",
    "<br>\n",
    "<br>\n",
    "We define the logistic loss without regularization as follow :\n",
    "$$\\hat{L_1}(w) = \\frac{1}{n}\\sum_{i=1}^n{\\log(1+e^{-y_i h_w(x_i)}}) \\ (1.1)$$\n",
    "For the case of LASSO the loss is defined as follow :\n",
    "$$\\hat{L_2}(w) = L_1(w) + \\lambda \\lVert w \\rVert_1 \\ (1.2)$$\n",
    "For the case of RIDGE the loss is defined as follow :\n",
    "$$\\hat{L_3}(w) = L_1(w) + \\lambda \\lVert w \\rVert_{2}^{2} \\ (1.3) $$\n",
    "<br>\n",
    "\n",
    "The functions $L_1$ and $L_3$ are differentiable and their derivative is given by :\n",
    "$$\n",
    "\\nabla L(\\left.w_{0}, w_{1}, \\ldots, w_{d}\\right)=\\left[\\begin{array}{c}\n",
    "\\dfrac{\\partial L}{\\partial w_0}(\\left.w_{0}, w_{1}, \\ldots, w_{d}\\right)\\\\\n",
    "\\dfrac{\\partial L}{\\partial w_1}(\\left.w_{0}, w_{1}, \\ldots, w_{d}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\dfrac{\\partial L}{\\partial w_d}(\\left.w_{0}, w_{1}, \\ldots, w_{d}\\right) \n",
    "\\end{array}\\right] \\ (1.4)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Let $ \\ l_i(w) = log(1+e^{-y_i h_w(x_i)})$, with the equation (1.1) that come down  to : $L_1(w) = \\frac{1}{n} \\sum_{i=1}^n{l_i(w)}$\n",
    "<br>\n",
    "We know that the derivative of $p(x) = log(1+e^{-u})$ with respect of $x$ and $u = u(x)$ is equal to : \n",
    "$p'(x) = \\frac{-u'(x)e^{-u(x)}} {1+e^{-u(x)}} = -u'(x)(1-\\sigma(u(x)) \\ (1.5)$\n",
    "<br>\n",
    "Using the relation (1.5) with $u(x) = -y h_w(x)$ we obtain :\n",
    "\n",
    "$$ \\dfrac{\\partial l_i}{\\partial w_j} =  \\dfrac{\\partial h_w(x_i)}{\\partial w_j} \\ast \\frac{-y_i e^{-y_i h_w(x_i)}} {1+e^{-y_i h_w(x_i)}} = -y_i\\dfrac{\\partial h_w(x_i)}{\\partial w_j}(1-\\sigma(y_i h_w(x_i)) (1.6)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ \\dfrac{\\partial h_w(x_i)}{\\partial w_j} = \\left\\{\n",
    "    \\begin{array}{ll}   \n",
    "        1 & \\mbox{if} j = 0 \\\\\n",
    "        x_{ij} & \\mbox{else}\n",
    "    \\end{array}\n",
    "\\right. \\ (1.7)\n",
    "\\\\\n",
    "$$\n",
    "<br>\n",
    "$x_{ij}$ is the j-th component of the vector $x_i$\n",
    "<br>\n",
    "So with the equations (1.6) and (1.7) we have :\n",
    "$$\n",
    "\\dfrac{\\partial l_i}{\\partial w_j} = \\left\\{\n",
    "    \\begin{array}{ll}   \n",
    "        -y_i(1-\\sigma(y_i h_w(x_i)) & \\mbox{if } j = 0 \\\\\n",
    "       -y_i(1-\\sigma(y_i h_w(x_i)) \\ast x_{ij} & \\mbox{else}\n",
    "    \\end{array}\n",
    "\\right.\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "We have : $$\\dfrac{\\partial L_1}{\\partial w_j} = \\frac{1}{n}\\sum_{i=1}^n{\\dfrac{\\partial l_i}{\\partial w_j}} \\left\\{\n",
    "    \\begin{array}{ll}   \n",
    "        \\frac{-\\sum_{i=1}^n{y_i(1-\\sigma(y_i h_w(x_i))}}{n} & \\mbox{if } j = 0 \\\\\n",
    "        \\frac{-\\sum_{i=1}^n{y_i(1-\\sigma(y_i h_w(x_i))x_{ij}}}{n} & \\mbox{else}\n",
    "    \\end{array} \\ (1.8)\n",
    "\\right.\n",
    "\\\\ \n",
    " $$\n",
    "\n",
    "For the case of RIDGE we have :\n",
    "$$\\dfrac{\\partial L_3}{\\partial w_j} = \\dfrac{\\partial L_1}{\\partial w_j} + \\lambda \\dfrac{\\partial \\lVert w \\rVert_{2}^{2}}{\\partial w_j} = \\left\\{\n",
    "    \\begin{array}{ll}   \n",
    "        \\frac{-\\sum_{i=1}^n{y_i(1-\\sigma(y_i h_w(x_i))}}{n} + 2 \\lambda w_0 & \\mbox{if } j = 0 \\\\\n",
    "        \\frac{-\\sum_{i=1}^n{y_i(1-\\sigma(y_i h_w(x_i))x_{ij}}}{n}+2\\lambda w_j & \\mbox{else}\n",
    "    \\end{array}\n",
    "\\right. \\ (1.9)\n",
    "\\\\ \n",
    " $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADMM (Alternating Direction Method for Multipliers)\n",
    "Reference :  [Admm-Distr-Stats)](https://fr.scribd.com/document/298976199/Admm-Distr-Stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving logistic regression problem without regularization\n",
    "In this second part we present the ADMM (Alternating Direction Method for Multipliers) approach to solve logistic regression for both cases that we present in the first part.\n",
    "We consider the loss function $L_1$ in equation (1.1) , the logistic regression problem can be express like that : \n",
    "$$ min_{w \\in \\mathbb{R}^{d+1}} \\frac{\\sum_{i=1}^n{\\log(1+e^{-y_i h_w(x_i)}})}{n}  \\ (2.1)$$\n",
    "This is an optimization problem without constraint, in ADMM form the problem (2.1) can be written as follow :\n",
    "$$min L_1(w)$$ $$ subject \\ to \\ w - z = 0 \\ (2.2)$$\n",
    "The augmented lagrangian is defined by :  $$L_\\rho(w, z, h) = L_1(w) + h^T (w-z) + \\frac{\\rho}{2}\\lVert w - z \\rVert_{2}^{2}$$ \n",
    "Using the ADMM algorithm we have the following iterations:\n",
    "$$w^{k+1} = argmin_{w} \\ L_\\rho(w, z^{k}, y^{k}) \\ (2.3)$$\n",
    "$$z^{k+1} = argmin_{z} \\ L_\\rho(w^{k+1}, z, y^{k}) \\ (2.4)$$\n",
    "$$y^{k+1} = y^{k} + \\rho (w^{k+1} - z^{k+1}) \\ (2.5)$$     \n",
    "Using the scaled form of ADDM algorithm with $u = \\frac{h}{\\rho}$ the equations (2.3), (2.4), (2.5) remain to :\n",
    "$$ w^{k+1} = argmin_{w} \\ L_1(w) + \\frac{\\rho}{2} \\lVert w - z^{k} + u^{k} \\rVert_{2}^{2} \\ (2.6)$$\n",
    "$$ z^{k+1} = argmin_{z} \\lVert w^{k+1} - z + u^{k} \\rVert_{2}^{2} \\ (2.7)$$\n",
    "$$u^{k+1} = u^{k} + w^{k+1} - z^{k+1} \\ (2.8)$$     \n",
    "In the first equation (2.6) the objective function is convex so we can use any kind of optimization method to solve it\n",
    "<br>\n",
    "In the second equation (2.7) there are not constraints with the variables so the minimum of this function is O and this is obtained when\n",
    "$$z = w^{k+1} + u^{k} \\ (2.9)$$\n",
    "Proof : To have the scaled form of the ADMM and obtain equations (2.5), (2.6), (2.7) we have the following lines :\n",
    "Let $ r = w-z$ and $u = \\frac{h}{\\rho}$ , we have :\n",
    "$$ h^T (w-z) + \\frac{\\rho}{2}\\lVert w - z \\rVert_{2}^{2} = h^T r + \\frac{\\rho}{2}\\lVert r\\rVert_{2}^{2}$$\n",
    "$$\\Longrightarrow h^T r + \\frac{\\rho}{2}\\lVert r\\rVert_{2}^{2} = \\rho u^T r + \\frac{\\rho}{2}\\lVert r\\rVert_{2}^{2}$$\n",
    "$$ \\Longrightarrow h^T r + \\frac{\\rho}{2}\\lVert r\\rVert_{2}^{2} = \\frac{\\rho}{2}(2u^T r + \\lVert r\\rVert_{2}^{2}$$\n",
    "$$ \\Longrightarrow h^T r + \\frac{\\rho}{2}\\lVert r\\rVert_{2}^{2} = \\frac{\\rho}{2}(\\lVert u + r\\rVert_{2}^{2} - \\lVert u \\rVert_{2}^{2})$$\n",
    "$$ \\Longrightarrow h^T r + \\frac{\\rho}{2}\\lVert r\\rVert_{2}^{2} = \\frac{\\rho}{2}(\\lVert u + w-z\\rVert_{2}^{2} - \\lVert u \\rVert_{2}^{2}) \\ (2.10)$$ \n",
    "With equation 2.10 the augmented lagrangian is now given by :\n",
    "$$L_\\rho(w, z, u) = L_1(w) +\\frac{\\rho}{2}(\\lVert u + w-z\\rVert_{2}^{2} - \\lVert u \\rVert_{2}^{2})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving logistic regression + RIDGE\n",
    "\n",
    "We consider the loss function $L_3$ in equation (1.3) , the logistic regression +ridge problem can be express like that : \n",
    "$$ min_{w \\in \\mathbb{R}^{d+1}} L_3(w) \\ (3.1)$$\n",
    "\n",
    "This is an optimization with constraint, in the ADMM form the proble 2.1 can be written as :\n",
    "$$min \\ L_1(w) + \\lambda \\lVert z \\rVert_{2}^{2} \\\\ subject \\ to \\ w - z = 0 \\ (3.2)$$\n",
    "the augmented lagrangian is defined by :  $$L_\\rho(w, z, h) = L_1(w)+ \\lambda \\lVert z \\rVert_{2}^{2} + h^T (w-z) + \\frac{\\rho}{2}\\lVert w - z \\rVert_{2}^{2}$$ \n",
    "  \n",
    "Using the scaled form of ADDM that remains to :\n",
    "$$ w^{k+1} = argmin_{w} \\ L_1(w) + \\rho \\lVert w - z^{k} + u^{k} \\rVert_{2}^{2} \\ (3.3)$$\n",
    "$$ z^{k+1} = argmin_{z} \\ \\lambda \\lVert z \\rVert_{2}^{2}+ \\frac{\\rho}{2} \\lVert w^{k+1} - z + u^{k} \\rVert_{2}^{2} \\ (3.4)$$\n",
    "$$u^{k+1} = u^{k} + w^{k+1} - z^{k+1} \\ (3.5)$$     \n",
    "In the first equation (3.3) the objective function is convex so we can use any kind of optimization method to solve it\n",
    "<br>\n",
    "In the second equation (3.5) this is a least square problem with z as variable we can use a solver for this problem, we have a quadratic function an by solving the gradient of this function equal to the null vector we have a solution defined by :\n",
    "$$f(z) = \\lambda \\lVert z \\rVert_{2}^{2}+ \\frac{\\rho}{2} \\lVert z - w^{k+1} - u^{k} \\rVert_{2}^{2} $$\n",
    "$$\\nabla f(z) = 2\\lambda z + \\rho (z-w^{k+1}-u^{k})$$\n",
    "$$\\nabla f(z) = 0 \\Longrightarrow z =  \\frac{\\rho (w^{k+1} + u^{k})}{2\\lambda + \\rho} (3.6) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving logistic regression + LASSO with ADMM\n",
    "In this second part we present the ADMM (Alternating Direction Method for Multipliers) approach to solve logistic regression +LASSO.\n",
    "We consider the loss function $L_2$ in equation (1.2) , the logistic regression + lasso problem can be express like that : \n",
    "$$ min_{w \\in \\mathbb{R}^{d+1}} L_2(w)  \\ (4.1)$$\n",
    "This is an optimization with constraint, using ADMM form we can written the equation 4.1 as:\n",
    "$$min \\ L_1(w) + \\lambda \\lVert z \\rVert_{1}$$ $$ subject \\ to \\ w - z = 0 \\ (4.2)$$\n",
    "the augmented lagrangian is defined by :  $$L_\\rho(w, z, h) = L_1(w)+ \\lambda \\lVert z \\rVert_{1} + h^T (w-z) + \\frac{\\rho}{2}\\lVert w - z \\rVert_{2}^{2}$$  \n",
    "Using the scaled form of ADDM that remains to :\n",
    "$$ w^{k+1} = argmin_{w} \\ L_1(w) + \\rho \\lVert w - z^{k} + u^{k} \\rVert_{2}^{2} \\ (4.3)$$\n",
    "$$ z^{k+1} = argmin_{z} \\ \\lambda \\lVert z \\rVert_{1}+ \\frac{\\rho}{2} \\lVert w^{k+1} - z + u^{k} \\rVert_{2}^{2} \\ (4.4)$$\n",
    "$$u^{k+1} = u^{k} + w^{k+1} - z^{k+1} \\ (4.5)$$     \n",
    "In the first equation (4.3) the objective function is convex so we can use any kind of optimization method to solve it\n",
    "<br>\n",
    "In the second equation (4.5) the function is not diffentiable so we can solve this problem using a soft thresholding defined as follow :\n",
    "$$f(z) = \\lambda \\lVert z \\rVert_{1}+ \\frac{\\rho}{2} \\lVert w^{k+1} - z + u^{k} \\rVert_{2} $$\n",
    "$$ z_i = argmin_{z_i} \\ f(z_i) \\ \\forall i \\in \\{0, 1,..., d\\}$$\n",
    "So this give us  \n",
    "$$ \\forall i \\in \\{0, 1,..., d\\}, \\ z_i = S_{\\frac{\\lambda}{\\rho}}(w_{i}^{k+1} + u_{i}^{k}) \\ (4.6)$$\n",
    "Where $$ S_a(v) = \\left\\{\n",
    "    \\begin{array}{ll}   \n",
    "        v-a & \\mbox{if } v > a \\\\\n",
    "        0 & \\mbox{if } \\lvert v \\rvert \\le a \\\\\n",
    "        a-v & \\mbox{else}\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stoping criterion\n",
    "Let $r^{k}  = w^{k} - z^k$ and $s^k = -\\rho (z^k - z^{k-1})$ \n",
    "<br>\n",
    "The stopping criterion of ADMM algorithm is : \n",
    "$$ \\lVert r^k \\rVert_{2}^{2} \\le \\epsilon^{prim} \\ and  \\ \\lVert s^k \\rVert_{2}^{2} \\le \\epsilon^{dual}$$\n",
    "With :\n",
    "$$ \\epsilon^{prim}= \\sqrt{d+1} \\epsilon^{abs} + \\epsilon^{rel}max(\\lVert w^k \\rVert_{2}^{2}, \\lVert z^k \\rVert_{2}^{2}) \\ (5.1)$$\n",
    "<br>\n",
    "$$ \\epsilon^{dual}= \\sqrt{d+1} \\epsilon^{abs} + \\epsilon^{rel} \\lVert u^k \\rVert_{2}^{2} \\ (5.2)$$\n",
    "\n",
    "In the equations 5.1 and 5.2 $\\epsilon^{abs} \\ and \\ \\epsilon^{rel}$ are fixed by the user "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_w <- function(w, x){\n",
    "    p <-length(w)\n",
    "    return(w[1] + x%*%w[2:p])\n",
    "}\n",
    "\n",
    "sigmoid <- function(x){\n",
    "    return(1/(1+exp(-x)))\n",
    "}\n",
    "\n",
    "## the function l1 compute the objective function of equation 2.6\n",
    "l1 <- function(w, X, y, n, d, u, z, rho){\n",
    "    f <- 0.0\n",
    "    for(i in 1:n)\n",
    "    {\n",
    "        f <- f + log(1 + exp(-y[i]*h_w(w,X[i,]))) \n",
    "    }\n",
    "    return(f + 0.5*rho*norm(w-z+u, type = \"2\")^2)\n",
    "}\n",
    "## The function grad_l1 compute the gradient of the objective function of equation 2.6\n",
    "grad_l1 <- function(w, X, y, n, d, u, z, rho){\n",
    "    grad <- numeric(d+1)\n",
    "    for(j in 1:d+1)\n",
    "    {\n",
    "        s <- 0\n",
    "        for(i in 1:n)\n",
    "        {\n",
    "           \n",
    "            if(j==1)\n",
    "            {\n",
    "                s <- s-y[i]*(1-sigmoid(y[i]*h_w(w, X[i,]))) + rho*(w[j]-z[j]+u[j])\n",
    "            }\n",
    "            else\n",
    "            {\n",
    "                s <- s-y[i]*(1-sigmoid(y[i]*h_w(w, X[i,])))*X[i,j-1] + rho*(w[j]-z[j]+u[j])\n",
    "            }\n",
    "\n",
    "        }\n",
    "        grad[j] <- s\n",
    "    }\n",
    "    return(grad)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADMM for logistic regression without constraints\n",
    "addm_logistic_regression <- function(X, y, rho, eps_abs, eps_rel, maxEp){\n",
    "    ##MaxExp is the total number of iterations that we choose\n",
    "    n  <- nrow(X)\n",
    "    d <- ncol(X)\n",
    "    w <- numeric(d+1)\n",
    "    z <- numeric(d+1)\n",
    "    u <- numeric(d+1)\n",
    "    k <- 1\n",
    "    while (k<=maxEp)\n",
    "    {\n",
    "        optimised <- optim(par = w, X, y, n, d, u, z, rho, fn = l1, gr = grad_l1, method=\"L-BFGS-B\" )\n",
    "        ## optimised is the function that solves equation 2.6 using newton method w_new at this iteration is w_(k+1)\n",
    "        ##optimised$par is the vector resulting of this newton optimisation method\n",
    "        w_new <- optimised$par \n",
    "        z_new <- w_new + u #equation 2.9\n",
    "        u_new <- u + w_new - z_new #equation 2.10\n",
    "        r <- w_new-z_new\n",
    "        s <- -rho*(z_new-z)\n",
    "        eps_prim <- sqrt(d+1)*eps_abs + eps_rel*max(norm(z_new, type = \"2\"), norm(w_new, type =\"2\")) # equation 5.1\n",
    "        eps_dual <- sqrt(d+1)*eps_abs + eps_rel*norm(u_new, type = \"2\") # equation 5.2\n",
    "        z <- z_new\n",
    "        w <- w_new\n",
    "        u <- u_new\n",
    "        if((norm(r, type =\"2\")<=eps_prim) & (norm(s, type = \"2\")<=eps_dual)){\n",
    "            # We verify if the stopping criteria is satisfied\n",
    "            break\n",
    "        }\n",
    "        k <- k+1\n",
    "        print(k)\n",
    "    }\n",
    "    return(w)  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADMM for logistic regression + ridge\n",
    "addm_logistic_regression_ridge <- function(X, y, rho, lambda, eps_abs, eps_rel, maxEp){\n",
    "    ##lambda is the penalized coefficient\n",
    "    n  <- nrow(X)\n",
    "    d <- ncol(X)\n",
    "    w <- numeric(d+1)\n",
    "    z <- numeric(d+1)\n",
    "    u <- numeric(d+1)\n",
    "    k <- 1\n",
    "    while (k<=maxEp)\n",
    "    {\n",
    "        optimised <- optim(par = w, X, y, n, d, u, z, rho, fn = l1, gr = grad_l1, method=\"L-BFGS-B\" )\n",
    "        w_new <- optimised$par\n",
    "        z_new <- rho(w_new + u)/(2*lambda + rho) #equation 3.6\n",
    "        u_new <- u + w_new - z_new\n",
    "        r <- w_new - z_new\n",
    "        s <- -rho*(z_new - z)\n",
    "        eps_prim <- sqrt(d+1)*eps_abs + eps_rel*max(norm(z_new, type = \"2\"), norm(w_new, type =\"2\"))\n",
    "        eps_dual <- sqrt(d+1)*eps_abs + eps_rel*norm(u_new, type = \"2\")\n",
    "        z <- z_new\n",
    "        w <- w_new\n",
    "        u <- u_new\n",
    "        if((norm(r, type =\"2\")<=eps_prim) & (norm(s, type = \"2\")<=eps_dual)){\n",
    "            # We verify if the stopping criteria is satisfied\n",
    "            break\n",
    "        }\n",
    "        k <- k+1\n",
    "    }\n",
    "    return(w)  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function allows us to compute elements of equation 4.6\n",
    "soft_threshold <- function(a, v){\n",
    "    result <- 0\n",
    "    if(v>a){\n",
    "        result <- v-a\n",
    "    }\n",
    "    if(abs(v)<=a){\n",
    "        result <- 0\n",
    "    }\n",
    "    else{\n",
    "        result <- a-v\n",
    "    }\n",
    "    return(result)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "addm_logistic_regression_lasso <- function(X, y, rho, lambda, eps_abs, eps_rel, maxEp){\n",
    "    ##lambda is the penalized coefficient\n",
    "    n  <- nrow(X)\n",
    "    d <- ncol(X)\n",
    "    w <- numeric(d+1)\n",
    "    z <- numeric(d+1)\n",
    "    u <- numeric(d+1)\n",
    "    k <- 1\n",
    "    while (k<=maxEp)\n",
    "    {\n",
    "        optimised <- optim(par = w, X, y, n, d, u, z, rho, fn = l1, gr = grad_l1, method=\"L-BFGS-B\" )\n",
    "        w_new <- optimised$par\n",
    "        z_new <- numeric(d+1)\n",
    "        for(i in 1:d+1){\n",
    "            z_new[i] <- soft_threshold(lambda/rho, w_new[i] + u_new[i]) #equation 4.6 to compute z_update\n",
    "        }\n",
    "        u_new <- u + w_new - z_new\n",
    "        r <- w_new - z_new\n",
    "        s <- -rho*(z_new - z)\n",
    "        eps_prim <- sqrt(d+1)*eps_abs + eps_rel*max(norm(z_new, type = \"2\"), norm(w_new, type =\"2\"))\n",
    "        eps_dual <- sqrt(d+1)*eps_abs + eps_rel*norm(u_new, type = \"2\")\n",
    "        z <- z_new\n",
    "        w <- w_new\n",
    "        u <- u_new\n",
    "        if((norm(r, type =\"2\")<=eps_prim) & (norm(s, type = \"2\")<=eps_dual)){\n",
    "            # We verify if the stopping criteria is satisfied\n",
    "            break\n",
    "        }\n",
    "        k <- k+1\n",
    "    }\n",
    "    return(w)  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar <- read.csv('sonar.txt', header =FALSE)\n",
    "levels(sonar$V61) <- c(-1,1)\n",
    "set.seed(42)\n",
    "rows <- sample(nrow(sonar))\n",
    "sonar <- sonar[rows,]\n",
    "X <- as.matrix(sonar[, 1:60])\n",
    "y <- as.vector(sonar[, 61])\n",
    "y <- as.numeric(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 2\n",
      "[1] 3\n",
      "[1] 4\n",
      "[1] 5\n",
      "[1] 6\n",
      "[1] 7\n",
      "[1] 8\n",
      "[1] 9\n",
      "[1] 10\n",
      "[1] 11\n",
      "[1] 12\n",
      "[1] 13\n",
      "[1] 14\n",
      "[1] 15\n",
      "[1] 16\n",
      "[1] 17\n",
      "[1] 18\n",
      "[1] 19\n",
      "[1] 20\n",
      "[1] 21\n",
      "[1] 22\n",
      "[1] 23\n",
      "[1] 24\n",
      "[1] 25\n",
      "[1] 26\n",
      "[1] 27\n",
      "[1] 28\n",
      "[1] 29\n",
      "[1] 30\n",
      "[1] 31\n",
      "[1] 32\n",
      "[1] 33\n",
      "[1] 34\n",
      "[1] 35\n",
      "[1] 36\n",
      "[1] 37\n",
      "[1] 38\n",
      "[1] 39\n",
      "[1] 40\n",
      "[1] 41\n",
      "[1] 42\n",
      "[1] 43\n",
      "[1] 44\n",
      "[1] 45\n",
      "[1] 46\n",
      "[1] 47\n",
      "[1] 48\n",
      "[1] 49\n",
      "[1] 50\n",
      "[1] 51\n",
      "[1] 52\n",
      "[1] 53\n",
      "[1] 54\n",
      "[1] 55\n",
      "[1] 56\n",
      "[1] 57\n",
      "[1] 58\n",
      "[1] 59\n",
      "[1] 60\n",
      "[1] 61\n",
      "[1] 62\n",
      "[1] 63\n",
      "[1] 64\n",
      "[1] 65\n",
      "[1] 66\n",
      "[1] 67\n",
      "[1] 68\n",
      "[1] 69\n",
      "[1] 70\n",
      "[1] 71\n",
      "[1] 72\n",
      "[1] 73\n",
      "[1] 74\n",
      "[1] 75\n",
      "[1] 76\n",
      "[1] 77\n",
      "[1] 78\n",
      "[1] 79\n",
      "[1] 80\n",
      "[1] 81\n",
      "[1] 82\n",
      "[1] 83\n",
      "[1] 84\n",
      "[1] 85\n",
      "[1] 86\n",
      "[1] 87\n",
      "[1] 88\n",
      "[1] 89\n",
      "[1] 90\n",
      "[1] 91\n",
      "[1] 92\n",
      "[1] 93\n",
      "[1] 94\n",
      "[1] 95\n",
      "[1] 96\n",
      "[1] 97\n",
      "[1] 98\n",
      "[1] 99\n",
      "[1] 100\n",
      "[1] 101\n"
     ]
    }
   ],
   "source": [
    "w_test <- addm_logistic_regression(X = X, y = y, rho=1, eps_abs=1e-4,eps_rel=1e-2, maxEp = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>0.8991088</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{l}\n",
       "\t 0.8991088\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 0.8991088 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]     \n",
       "[1,] 0.8991088"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
